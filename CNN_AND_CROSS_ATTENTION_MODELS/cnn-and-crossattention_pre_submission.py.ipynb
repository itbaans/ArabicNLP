{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12701773,"sourceType":"datasetVersion","datasetId":8027417}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:07:52.595609Z","iopub.execute_input":"2025-08-14T09:07:52.595902Z","iopub.status.idle":"2025-08-14T09:07:52.928600Z","shell.execute_reply.started":"2025-08-14T09:07:52.595885Z","shell.execute_reply":"2025-08-14T09:07:52.927821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom PIL import Image\nfrom transformers import (\n    CLIPProcessor, CLIPModel, CLIPVisionModel,\n    AutoModel, AutoProcessor, AutoTokenizer,\n    AutoModelForMaskedLM\n)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nimport numpy as np\nfrom collections import Counter, defaultdict\nimport os\nimport json\nfrom datetime import datetime\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:07:52.930197Z","iopub.execute_input":"2025-08-14T09:07:52.930518Z","iopub.status.idle":"2025-08-14T09:08:17.925821Z","shell.execute_reply.started":"2025-08-14T09:07:52.930500Z","shell.execute_reply":"2025-08-14T09:08:17.925076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNNFusionLayer(nn.Module):\n    \"\"\"CNN-style fusion layer with 1D convolutions and enhanced regularization\"\"\"\n    def __init__(self, input_dim, output_dim, kernel_size=3, dropout=0.2):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        \n        # 1D convolutions with reduced parameters\n        self.img_conv = nn.Conv1d(input_dim, output_dim, kernel_size, padding=kernel_size//2, padding_mode='zeros')\n        self.txt_conv = nn.Conv1d(input_dim, output_dim, kernel_size, padding=kernel_size//2, padding_mode='zeros')\n        \n        # Simplified cross-modal interaction\n        self.cross_conv = nn.Conv1d(output_dim * 2, output_dim, 1)\n        \n        # Normalization with affine transforms disabled\n        self.img_norm = nn.BatchNorm1d(output_dim, affine=False)\n        self.txt_norm = nn.BatchNorm1d(output_dim, affine=False)\n        self.cross_norm = nn.BatchNorm1d(output_dim, affine=False)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.activation = nn.ReLU()\n\n    def forward(self, img_embeddings, txt_embeddings, txt_mask=None):\n        img_t = img_embeddings.transpose(1, 2)  # [B, D, L_img]\n        txt_t = txt_embeddings.transpose(1, 2)  # [B, D, L_txt]\n        \n        img_conv = self.activation(self.img_norm(self.img_conv(img_t)))\n        txt_conv = self.activation(self.txt_norm(self.txt_conv(txt_t)))\n        \n        img_pooled = F.interpolate(\n            img_conv, \n            size=txt_conv.size(2), \n            mode='linear', \n            align_corners=False\n        )  # Now [B, D, L_txt]\n        \n        if txt_mask is not None:\n            mask = txt_mask.unsqueeze(1).float()  # [B, 1, L_txt]\n            txt_conv = txt_conv * mask\n        \n        combined = torch.cat([img_pooled, txt_conv], dim=1)\n        fused = self.activation(self.cross_norm(self.cross_conv(combined)))\n        \n        img_out = img_conv.transpose(1, 2)\n        txt_out = fused.transpose(1, 2)\n        \n        return self.dropout(img_out), self.dropout(txt_out)\n\nclass CNNMultiModalModel(nn.Module):\n    \"\"\"Optimized multimodal model with anti-overfitting features\"\"\"\n    def __init__(self, num_labels=2, freeze_backbones=True,\n                 layer_dims=[512, 256],  # Reduced layers\n                 kernel_sizes=[3, 3],     # Smaller kernels\n                 pool_type='avg'):\n        super().__init__()\n        \n        # Load pretrained models\n        self.vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.text_model = AutoModel.from_pretrained(\"UBC-NLP/MARBERT\")\n        \n        # Selective unfreezing (last 2 layers)\n        if freeze_backbones:\n            # Freeze all layers first\n            for param in self.vision_model.parameters():\n                param.requires_grad = False\n            for param in self.text_model.parameters():\n                param.requires_grad = False\n            \n            # Unfreeze last 2 vision layers\n            for layer in self.vision_model.vision_model.encoder.layers[-2:]:\n                for param in layer.parameters():\n                    param.requires_grad = True\n            \n            # Unfreeze last 2 text layers\n            for layer in self.text_model.encoder.layer[-2:]:\n                for param in layer.parameters():\n                    param.requires_grad = True\n        \n        # Backbone output regularization\n        self.vision_dropout = nn.Dropout(0.3)\n        self.text_dropout = nn.Dropout(0.3)\n        \n        # Create shallower fusion layers with higher dropout\n        self.fusion_layers = nn.ModuleList()\n        for i in range(len(layer_dims)):\n            input_dim = self.vision_model.config.hidden_size if i == 0 else layer_dims[i-1]\n            layer = CNNFusionLayer(\n                input_dim=input_dim,\n                output_dim=layer_dims[i],\n                kernel_size=kernel_sizes[i],\n                dropout=0.2 + i * 0.15  # Increasing dropout\n            )\n            self.fusion_layers.append(layer)\n            \n        # Final pooling\n        self.pool_type = pool_type\n        final_dim = layer_dims[-1]\n        \n        # Simplified classifier with higher regularization\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(final_dim * 2, final_dim),\n            nn.GELU(),\n            nn.LayerNorm(final_dim),\n            nn.Dropout(0.4),\n            nn.Linear(final_dim, num_labels)\n        )\n\n    def pool_features(self, img_features, txt_features, txt_mask=None):\n        \"\"\"Pool sequence features with mask support\"\"\"\n        # Image pooling (no mask needed)\n        img_pooled = img_features.mean(dim=1)\n        \n        # Text pooling with mask\n        if txt_mask is not None:\n            mask_expanded = txt_mask.unsqueeze(-1).float()\n            txt_pooled = (txt_features * mask_expanded).sum(dim=1)\n            txt_pooled /= mask_expanded.sum(dim=1).clamp(min=1e-7)\n        else:\n            txt_pooled = txt_features.mean(dim=1)\n            \n        return img_pooled, txt_pooled\n\n    def forward(self, pixel_values, input_ids, attention_mask):\n        # Extract backbone features with regularization\n        vision_outputs = self.vision_model(pixel_values=pixel_values)\n        img_embeddings = self.vision_dropout(vision_outputs.last_hidden_state)\n        \n        text_outputs = self.text_model(\n            input_ids=input_ids, \n            attention_mask=attention_mask\n        )\n        txt_embeddings = self.text_dropout(text_outputs.last_hidden_state)\n        \n        # Progressive fusion\n        for layer in self.fusion_layers:\n            img_embeddings, txt_embeddings = layer(\n                img_embeddings, \n                txt_embeddings,\n                attention_mask\n            )\n        \n        # Final pooling\n        img_pooled, txt_pooled = self.pool_features(\n            img_embeddings, \n            txt_embeddings, \n            attention_mask\n        )\n        \n        # Classification\n        fused = torch.cat([img_pooled, txt_pooled], dim=-1)\n        return self.classifier(fused)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:08:17.926636Z","iopub.execute_input":"2025-08-14T09:08:17.927268Z","iopub.status.idle":"2025-08-14T09:08:17.941708Z","shell.execute_reply.started":"2025-08-14T09:08:17.927241Z","shell.execute_reply":"2025-08-14T09:08:17.941099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CrossAttentionFusion(nn.Module):\n    \"\"\"\n    Cross-attention fusion with pooling over token-level attended outputs.\n    Supports 'mean', 'max', or self-attention pooling.\n    \"\"\"\n    def __init__(self, hidden_dim, num_heads, pool_mode='max'):\n        super().__init__()\n        assert pool_mode in ['mean', 'max', 'attention'], \"pool_mode must be 'mean', 'max', or 'attention'\"\n        self.pool_mode = pool_mode\n\n        self.text_to_image_attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim, num_heads=num_heads, batch_first=True\n        )\n        self.image_to_text_attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim, num_heads=num_heads, batch_first=True\n        )\n\n        if pool_mode == 'attention':\n            # learnable pooling vector for image and text\n            self.img_pool_query = nn.Parameter(torch.randn(1, 1, hidden_dim))\n            self.txt_pool_query = nn.Parameter(torch.randn(1, 1, hidden_dim))\n            # use dedicated single-head attention modules for pooling\n            self.img_pool_attn = nn.MultiheadAttention(\n                embed_dim=hidden_dim, num_heads=1, batch_first=True\n            )\n            self.txt_pool_attn = nn.MultiheadAttention(\n                embed_dim=hidden_dim, num_heads=1, batch_first=True\n            )\n\n    def forward(self, image_embeddings, text_embeddings, text_attention_mask=None):\n        # Cross-attention\n        attended_image, _ = self.text_to_image_attention(\n            query=image_embeddings,\n            key=text_embeddings,\n            value=text_embeddings,\n            key_padding_mask=(text_attention_mask == 0) if text_attention_mask is not None else None\n        )\n        attended_text, _ = self.image_to_text_attention(\n            query=text_embeddings,\n            key=image_embeddings,\n            value=image_embeddings\n        )\n\n        # Pooling\n        if self.pool_mode == 'mean':\n            image_feat = attended_image.mean(dim=1)  # [B, D]\n            text_feat  = attended_text.mean(dim=1)   # [B, D]\n\n        elif self.pool_mode == 'max':\n            image_feat, _ = attended_image.max(dim=1)  # [B, D]\n            text_feat, _  = attended_text.max(dim=1)   # [B, D]\n\n        else:  # attention pooling\n            B = image_embeddings.size(0)\n            # ensure pool queries are on same device as inputs\n            img_q = self.img_pool_query.expand(B, -1, -1).to(image_embeddings.device)  # [B,1,D]\n            txt_q = self.txt_pool_query.expand(B, -1, -1).to(text_embeddings.device)    # [B,1,D]\n\n            # attend to get pooled vector using module-level attentions\n            pooled_img, _ = self.img_pool_attn(\n                query=img_q, key=attended_image, value=attended_image\n            )  # [B,1,D]\n            pooled_txt, _ = self.txt_pool_attn(\n                query=txt_q, key=attended_text, value=attended_text\n            )  # [B,1,D]\n\n            image_feat = pooled_img[:, 0, :]  # [B, D]\n            text_feat  = pooled_txt[:, 0, :]  # [B, D]\n\n        # Concatenate pooled features\n        fused = torch.cat([image_feat, text_feat], dim=-1)  # [B, 2D]\n        return fused\n\n\nclass AdvancedFusionModel(nn.Module):\n    def __init__(self, num_labels=2, freeze_backbones=True, pool='max'):\n        super().__init__()\n        self.vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.text_model = AutoModel.from_pretrained(\"UBC-NLP/MARBERT\")\n\n        if freeze_backbones:\n            for param in self.vision_model.parameters():\n                param.requires_grad = False\n            for param in self.text_model.parameters():\n                param.requires_grad = False\n    \n        vision_embedding_dim = self.vision_model.config.hidden_size\n        text_embedding_dim = self.text_model.config.hidden_size\n        \n        # Assuming both dimensions are the same, which they are (768)\n        assert vision_embedding_dim == text_embedding_dim\n\n        self.fusion = CrossAttentionFusion(hidden_dim=vision_embedding_dim, num_heads=4, pool_mode=pool)\n\n        # The concatenated dimension from the fusion block\n        concatenated_dim = vision_embedding_dim * 2\n        self.proj = nn.Sequential(\n            nn.Dropout(0.4),\n            nn.Linear(concatenated_dim, 512)\n        )\n        \n        self.classification_head = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(512, num_labels)\n        )\n\n    def forward(self, pixel_values, input_ids, attention_mask, extract_features=False):\n        vision_outputs = self.vision_model(pixel_values=pixel_values)\n        image_embeddings = vision_outputs.last_hidden_state # [batch_size, num_patches+1, hidden_dim]\n\n        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeddings = text_outputs.last_hidden_state # [batch_size, seq_len, hidden_dim]\n\n        fused_features = self.fusion(image_embeddings, text_embeddings, attention_mask)\n        proj_features = self.proj(fused_features)\n\n        logits = self.classification_head(proj_features)\n        \n        if extract_features:\n            return proj_features\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:08:17.943099Z","iopub.execute_input":"2025-08-14T09:08:17.943335Z","iopub.status.idle":"2025-08-14T09:08:18.025241Z","shell.execute_reply.started":"2025-08-14T09:08:17.943315Z","shell.execute_reply":"2025-08-14T09:08:18.024349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Prop2HateMemeDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset for the Prop2Hate-Meme dataset.\n    \"\"\"\n    def __init__(self, jsonl_path, image_dir, clip_processor, text_tokenizer, max_len=128,\n                 is_labels=True):\n        \"\"\"\n        Args:\n            jsonl_path (str): Path to the .jsonl file (e.g., 'train.jsonl').\n            image_dir (str): Directory where the 'images/' folder is located.\n            clip_processor: The processor for CLIP images.\n            text_tokenizer: The tokenizer for MARBERT text.\n            max_len (int): Maximum sequence length for tokenization.\n        \"\"\"\n        self.data = load_dataset('json', data_files=jsonl_path)['train']\n        self.image_dir = image_dir\n        self.clip_processor = clip_processor\n        self.text_tokenizer = text_tokenizer\n        self.max_len = max_len\n        self.is_labels = is_labels\n\n        \n        # Map string labels to integers\n        # self.label_map = {\"not-hateful\": 0, \"hateful\": 1}\n        if self.is_labels and 'label' in self.data.column_names:\n            self.labels = self.data['label']\n        else:\n            self.labels = None\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # --- Load and Process Image ---\n        # --- Fix possible './' in img_path ---\n        img_path = item['img_path']\n        img_path = os.path.basename(img_path)\n    \n        image_path = os.path.join(self.image_dir, img_path)\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n            # Process image using CLIP's processor\n            image_processed = self.clip_processor(images=image, return_tensors=\"pt\")\n            pixel_values = image_processed['pixel_values'].squeeze(0) # Remove batch dim\n        except FileNotFoundError:\n            print(f\"Warning: Image not found at {image_path}. Using a dummy image.\")\n            # Provide a dummy tensor if an image is missing\n            pixel_values = torch.zeros((3, 224, 224))\n\n\n        # --- Load and Process Text ---\n        text = item['text']\n        # Tokenize text using MARBERT's tokenizer\n        text_tokenized = self.text_tokenizer(\n            text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        input_ids = text_tokenized['input_ids'].squeeze(0) # Remove batch dim\n        attention_mask = text_tokenized['attention_mask'].squeeze(0) # Remove batch dim\n        id_it = item['id']\n\n        #label = torch.tensor(item['label'], dtype=torch.long)\n\n        if not self.is_labels:\n            return {\n                'id': id_it,\n                'pixel_values': pixel_values,\n                'input_ids': input_ids,\n                'attention_mask': attention_mask\n            }\n        else:\n            label = torch.tensor(item['label'], dtype=torch.long)\n            return {\n                'pixel_values': pixel_values,\n                'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'label': label\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:08:18.026120Z","iopub.execute_input":"2025-08-14T09:08:18.026376Z","iopub.status.idle":"2025-08-14T09:08:18.041614Z","shell.execute_reply.started":"2025-08-14T09:08:18.026356Z","shell.execute_reply":"2025-08-14T09:08:18.040817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATASET_DIR = \"/kaggle/input/qcri-final-folder/QCRI_ARABIC_FOLDER/images\"\nTRAIN_FILE = \"/kaggle/input/qcri-final-folder/QCRI_ARABIC_FOLDER/new_train.jsonl\"\nTEST_FILE = \"/kaggle/input/qcri-final-folder/QCRI_ARABIC_FOLDER/new_test.jsonl\"\nBATCH_SIZE = 32\nLEARNING_RATE = 2e-5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nUSE_FOCAL_LOSS = True  # Switch to Focal Loss for class imbalance\n\n# --- Initialize Model Components ---\nclip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nmarbert_tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n\n# --- Dataset & Weighted Sampler ---\ntrain_dataset = Prop2HateMemeDataset(\n    jsonl_path=TRAIN_FILE,\n    image_dir=DATASET_DIR,\n    clip_processor=clip_processor,\n    text_tokenizer=marbert_tokenizer\n)\n\n# Calculate class weights for sampler\nlabel_counts = Counter(train_dataset.labels)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    pin_memory=True  # Faster data transfer to GPU\n)\nprint(len(train_dataset))\n\nval_dataset = Prop2HateMemeDataset(\n    jsonl_path=TEST_FILE,\n    image_dir=DATASET_DIR,\n    clip_processor=clip_processor,\n    text_tokenizer=marbert_tokenizer\n)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\nprint(len(val_dataset))\n\n# Focal Loss for class imbalance (handles hard examples)\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        loss = (1 - pt) ** self.gamma * ce_loss\n        if self.alpha is not None:\n            loss = self.alpha[targets] * loss\n        return loss.mean()\n\n# Class-aware loss weighting\nloss_weights = torch.tensor([\n    len(train_dataset) / (2 * label_counts[0]), \n    len(train_dataset) / (2 * label_counts[1])\n], dtype=torch.float32).to(DEVICE)\n\ncriterion = FocalLoss(alpha=loss_weights) if USE_FOCAL_LOSS \\\n            else nn.CrossEntropyLoss(weight=loss_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:10:04.735834Z","iopub.execute_input":"2025-08-14T09:10:04.736455Z","iopub.status.idle":"2025-08-14T09:10:09.679116Z","shell.execute_reply.started":"2025-08-14T09:10:04.736425Z","shell.execute_reply":"2025-08-14T09:10:09.678333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from torch.utils.data import DataLoader, Subset\n\n# # Assuming your original datasets are called train_dataset and val_dataset\n# train_subset = Subset(train_dataset, list(range(100)))\n# val_subset = Subset(val_dataset, list(range(20)))\n\n# train_loader = DataLoader(train_subset,batch_size=BATCH_SIZE,\n#     shuffle=True,\n#     pin_memory=True)  # Faster data transfer to GPU)\n                         \n# val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:08:27.142654Z","iopub.execute_input":"2025-08-14T09:08:27.142964Z","iopub.status.idle":"2025-08-14T09:08:27.147487Z","shell.execute_reply.started":"2025-08-14T09:08:27.142939Z","shell.execute_reply":"2025-08-14T09:08:27.146909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom sklearn.metrics import classification_report\n\ndef train_model(\n    model,\n    train_loader,\n    dev_loader,\n    criterion,\n    device,\n    learning_rate=5e-5,\n    num_epochs=50,\n    early_stop_patience=15,\n    weight_decay=1e-5,\n    clip_value=1.0,\n    model_save_path='best_model.pth',\n    metrics_file_path='training_metrics.json'\n):\n    # Initialize metrics dictionary\n    metrics = {\n        'epochs': [],\n        'train_losses': [],\n        'val_f1_macro': [],\n        'best_f1': 0,\n        'best_epoch': 0,\n        'classification_reports': {}\n    }\n    \n    # Move model to target device\n    model = model.to(device)\n    \n    # Initialize optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    scheduler = ReduceLROnPlateau(\n        optimizer, \n        mode='max', \n        factor=0.5, \n        patience=2, \n        verbose=True\n    )\n    \n    best_f1 = 0\n    epochs_no_improve = 0\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        total_loss = 0\n\n        for i, batch in enumerate(train_loader):\n            # Move batch to device\n            pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n            input_ids = batch['input_ids'].to(device, non_blocking=True)\n            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n            labels = batch['label'].to(device)\n\n            # Forward pass and loss calculation\n            optimizer.zero_grad()\n            logits = model(pixel_values, input_ids, attention_mask)\n            loss = criterion(logits, labels)\n            \n            # Backpropagation with gradient clipping\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n            optimizer.step()\n\n            total_loss += loss.item()\n            if (i + 1) % 50 == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n        # Validation phase\n        model.eval()\n        all_preds, all_labels = [], []\n        \n        with torch.no_grad():\n            for batch in dev_loader:\n                pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n                input_ids = batch['input_ids'].to(device, non_blocking=True)\n                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n                labels = batch['label'].to(device)\n                \n                logits = model(pixel_values, input_ids, attention_mask)\n                preds = torch.argmax(logits, dim=1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n        \n        # Calculate metrics\n        f1 = f1_score(all_labels, all_preds, average='macro')\n        avg_train_loss = total_loss / len(train_loader)\n        \n        # Record metrics\n        metrics['epochs'].append(epoch + 1)\n        metrics['train_losses'].append(avg_train_loss)\n        metrics['val_f1_macro'].append(f1)\n        \n        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val F1: {f1:.4f}\")\n        \n        # Update learning rate\n        scheduler.step(f1)\n        \n        # Check for new best score\n        if f1 > best_f1:\n            best_f1 = f1\n            metrics['best_f1'] = best_f1\n            metrics['best_epoch'] = epoch + 1\n            epochs_no_improve = 0\n            \n            # Save model\n            torch.save(model.state_dict(), model_save_path)\n            \n            # Generate and save classification report\n            report = classification_report(\n                all_labels, \n                all_preds,\n                output_dict=True\n            )\n            metrics['classification_reports'][f'epoch_{epoch+1}'] = report\n            \n            # Save metrics to file\n            with open(metrics_file_path, 'w') as f:\n                json.dump(metrics, f, indent=4)\n                \n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve == early_stop_patience:\n                print(f\"Early stopping triggered at epoch {epoch+1}\")\n                # Save metrics before early stopping\n                with open(metrics_file_path, 'w') as f:\n                    json.dump(metrics, f, indent=4)\n                break\n        \n        # Save metrics after each epoch (in case of interruption)\n        with open(metrics_file_path, 'w') as f:\n            json.dump(metrics, f, indent=4)\n\n    # Load best model weights\n    print(\"Training complete. Loading best model for final evaluation.\")\n    model.load_state_dict(torch.load(model_save_path))\n    \n    return model, best_f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:08:27.148226Z","iopub.execute_input":"2025-08-14T09:08:27.148472Z","iopub.status.idle":"2025-08-14T09:08:27.177827Z","shell.execute_reply.started":"2025-08-14T09:08:27.148445Z","shell.execute_reply":"2025-08-14T09:08:27.177075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\n\ndef generate_predictions(model, test_loader, output_path=\"prediction.csv\", device='cuda'):\n    \"\"\"\n    Generate predictions using a trained model and save them to a CSV file.\n    \n    Args:\n        model: Trained PyTorch model\n        test_loader: DataLoader for test data\n        output_path (str): Path to save predictions CSV (default: \"prediction.csv\")\n        device (str): Device to use for inference ('cuda' or 'cpu')\n    \"\"\"\n    model.eval()\n    predictions = []\n    ids = []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            pixel_values = batch['pixel_values'].to(DEVICE)\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n    \n            # Get model outputs (logits)\n            logits = model(\n                pixel_values=pixel_values,\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            \n            # Since the model uses BCEWithLogitsLoss and the final layer has one output,\n            # a logit value > 0 indicates the positive class (1, \"hateful\").\n            preds = torch.argmax(logits, dim=1)\n            predictions.extend(preds.cpu().numpy())\n            \n            # Handle IDs (convert to list if tensor)\n            batch_ids = batch['id']\n            if isinstance(batch_ids, torch.Tensor):\n                batch_ids = batch_ids.tolist()\n            ids.extend(batch_ids)\n\n    # Convert numerical predictions to labels\n    label_map = {0: 'not-hate', 1: 'hate'}\n    pred_labels = [label_map[p] for p in predictions]\n\n    # Save to CSV\n    df = pd.DataFrame({\n        'id': ids,\n        'prediction': pred_labels\n    })\n    df.to_csv(output_path, index=False)\n    print(f\"Predictions saved to {output_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:08:27.178616Z","iopub.execute_input":"2025-08-14T09:08:27.178861Z","iopub.status.idle":"2025-08-14T09:08:27.193138Z","shell.execute_reply.started":"2025-08-14T09:08:27.178838Z","shell.execute_reply":"2025-08-14T09:08:27.192289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import (classification_report, roc_auc_score, \n                             f1_score, precision_score, recall_score, \n                             accuracy_score, confusion_matrix)\nimport numpy as np\n\ndef evaluate_classification_performance(true_labels_path, predictions_path):\n    \"\"\"\n    Evaluate classification performance by comparing true labels with predictions.\n    \n    Args:\n        true_labels_path (str): Path to CSV file containing true labels (columns: id, test_label)\n        predictions_path (str): Path to CSV file containing predictions (columns: id, prediction)\n    \"\"\"\n    # Load the data\n    test_df = pd.read_csv(true_labels_path)\n    pred_df = pd.read_csv(predictions_path)\n\n    # Verify the IDs match (just in case)\n    if not (test_df['id'] == pred_df['id']).all():\n        print(\"Warning: IDs don't match between files! Results may be invalid.\")\n        # Alternative approach if IDs don't match:\n        # merged = pd.merge(test_df, pred_df, on='id', how='inner')\n        # y_true = merged['test_label']\n        # y_pred = merged['prediction']\n    else:\n        y_true = test_df['testing_label']\n        y_pred = pred_df['prediction']\n\n    # For binary classification, convert to numerical if needed\n    label_map = {'hate': 1, 'not-hate': 0}\n    if y_true.dtype == 'object':\n        y_true_num = y_true.map(label_map)\n        y_pred_num = y_pred.map(label_map)\n    else:\n        y_true_num = y_true\n        y_pred_num = y_pred\n\n    # Calculate metrics\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=['not-hate', 'hate']))\n\n    print(\"\\nAdditional Metrics:\")\n    print(f\"Accuracy: {accuracy_score(y_true_num, y_pred_num):.4f}\")\n    print(f\"F1 Macro: {f1_score(y_true_num, y_pred_num, average='macro'):.4f}\")\n    print(f\"Precision Macro: {precision_score(y_true_num, y_pred_num, average='macro'):.4f}\")\n    print(f\"Recall Macro: {recall_score(y_true_num, y_pred_num, average='macro'):.4f}\")\n\n    # Confusion matrix\n    print(\"\\nConfusion Matrix:\")\n    print(confusion_matrix(y_true_num, y_pred_num))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:08:27.196293Z","iopub.execute_input":"2025-08-14T09:08:27.196619Z","iopub.status.idle":"2025-08-14T09:08:27.214540Z","shell.execute_reply.started":"2025-08-14T09:08:27.196590Z","shell.execute_reply":"2025-08-14T09:08:27.213507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AdvancedFusionModel(num_labels=2, freeze_backbones=True).to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:10:18.231213Z","iopub.execute_input":"2025-08-14T09:10:18.231491Z","iopub.status.idle":"2025-08-14T09:10:21.207956Z","shell.execute_reply.started":"2025-08-14T09:10:18.231471Z","shell.execute_reply":"2025-08-14T09:10:21.207105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model, best_f1 = train_model(\n    model=model,\n    train_loader=train_loader,\n    dev_loader=val_loader,\n    criterion=criterion,\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    num_epochs=30,\n    early_stop_patience=10,\n    model_save_path='best_cross_attention_model.pth',\n    metrics_file_path='training_metrics_cross_attention.json'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:10:24.035316Z","iopub.execute_input":"2025-08-14T09:10:24.035600Z","iopub.status.idle":"2025-08-14T09:13:11.009297Z","shell.execute_reply.started":"2025-08-14T09:10:24.035581Z","shell.execute_reply":"2025-08-14T09:13:11.008216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = CNNMultiModalModel(\n        layer_dims=[512, 256],\n        kernel_sizes=[3, 3],\n        pool_type='avg'\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:18:05.766130Z","iopub.execute_input":"2025-08-14T09:18:05.766418Z","iopub.status.idle":"2025-08-14T09:18:08.296124Z","shell.execute_reply.started":"2025-08-14T09:18:05.766398Z","shell.execute_reply":"2025-08-14T09:18:08.295506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model, best_f1 = train_model(\n    model=model,\n    train_loader=train_loader,\n    dev_loader=val_loader,\n    criterion=criterion,\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    num_epochs=30,\n    early_stop_patience=10,\n    model_save_path='best_cnn_fusion_model.pth',\n    metrics_file_path='training_metrics_cnn_fusion.json'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:18:12.022474Z","iopub.execute_input":"2025-08-14T09:18:12.022748Z","iopub.status.idle":"2025-08-14T09:21:07.395147Z","shell.execute_reply.started":"2025-08-14T09:18:12.022729Z","shell.execute_reply":"2025-08-14T09:21:07.394323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_dataset = Prop2HateMemeDataset(\n#     jsonl_path=\"/kaggle/input/qcri-final-folder/QCRI_ARABIC_FOLDER/task3_test_without_label.jsonl\",\n#     image_dir=DATASET_DIR,\n#     clip_processor=clip_processor,\n#     text_tokenizer=marbert_tokenizer,\n#     is_labels=False\n# )\n\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:13:17.983820Z","iopub.execute_input":"2025-08-14T09:13:17.984153Z","iopub.status.idle":"2025-08-14T09:13:18.595013Z","shell.execute_reply.started":"2025-08-14T09:13:17.984133Z","shell.execute_reply":"2025-08-14T09:13:18.594348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate_predictions(trained_model, test_loader, \"post_predictions.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:21:19.154265Z","iopub.execute_input":"2025-08-14T09:21:19.155138Z","iopub.status.idle":"2025-08-14T09:21:33.194377Z","shell.execute_reply.started":"2025-08-14T09:21:19.155110Z","shell.execute_reply":"2025-08-14T09:21:33.193525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Example usage:\n# evaluate_classification_performance(\n#     '/kaggle/input/qcri-final-folder/QCRI_ARABIC_FOLDER/task3_test_gold.txt',\n#     '/kaggle/working/post_predictions.csv'\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T09:21:36.575313Z","iopub.execute_input":"2025-08-14T09:21:36.575599Z","iopub.status.idle":"2025-08-14T09:21:36.612058Z","shell.execute_reply.started":"2025-08-14T09:21:36.575580Z","shell.execute_reply":"2025-08-14T09:21:36.611303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}