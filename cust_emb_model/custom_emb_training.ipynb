{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom PIL import Image\nfrom transformers import (\n    CLIPProcessor, CLIPModel, CLIPVisionModel,\n    AutoModel, AutoProcessor, AutoTokenizer,\n    AutoModelForMaskedLM\n)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nimport numpy as np\nfrom collections import Counter, defaultdict\nimport os\nimport json\nfrom datetime import datetime\nimport pandas as pd\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Bismillah\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CLIPArabic(nn.Module):\n    def __init__(self, image_encoder, text_encoder, proj_dim=512, freeze_encoders=True):\n        super().__init__()\n        self.img_enc = image_encoder\n        self.txt_enc = text_encoder\n\n        # Determine feature dimensions of encoders\n        if hasattr(self.img_enc, 'embed_dim'):\n            img_dim = self.img_enc.embed_dim\n        elif hasattr(self.img_enc, 'config') and hasattr(self.img_enc.config, 'hidden_size'):\n            img_dim = self.img_enc.config.hidden_size\n        else:\n            raise AttributeError(\n                \"Unable to infer image encoder output dimension; please specify explicitly or ensure encoder has 'embed_dim' or 'config.hidden_size'.\"\n            )\n\n        if hasattr(self.txt_enc, 'config') and hasattr(self.txt_enc.config, 'hidden_size'):\n            txt_dim = self.txt_enc.config.hidden_size\n        else:\n            raise AttributeError(\n                \"Unable to infer text encoder hidden size; ensure encoder has 'config.hidden_size'.\"\n            )\n\n        # Projection heads\n        self.img_proj = nn.Linear(img_dim, proj_dim)\n        self.txt_proj = nn.Linear(txt_dim, proj_dim)\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n\n        # Freeze encoders if requested\n        if freeze_encoders:\n            for param in self.img_enc.parameters():\n                param.requires_grad = False\n            for param in self.txt_enc.parameters():\n                param.requires_grad = False\n\n    def forward(self, pixel_values, input_ids, attention_mask):\n        # 1) Image encode\n        img_out = self.img_enc(pixel_values=pixel_values)\n        if isinstance(img_out, torch.Tensor):\n            img_feat = img_out\n        else:\n            # Prefer pooler_output if it exists and is not None\n            if hasattr(img_out, 'pooler_output') and img_out.pooler_output is not None:\n                img_feat = img_out.pooler_output\n            else:\n                img_feat = img_out.last_hidden_state[:, 0]\n    \n        # 2) Text encode\n        txt_out  = self.txt_enc(input_ids=input_ids, attention_mask=attention_mask)\n        txt_feat = txt_out.last_hidden_state[:, 0]\n    \n        # 3) Project & normalize\n        img_emb = F.normalize(self.img_proj(img_feat), dim=-1)\n        txt_emb = F.normalize(self.txt_proj(txt_feat), dim=-1)\n    \n        # 4) Return with temp\n        return img_emb, txt_emb, self.logit_scale.exp()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Prop2HateMemeDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset for the Prop2Hate-Meme dataset.\n    \"\"\"\n    def __init__(self, jsonl_path, image_dir, clip_processor, text_tokenizer, max_len=128,\n                 is_labels=False):\n        \"\"\"\n        Args:\n            jsonl_path (str): Path to the .jsonl file (e.g., 'train.jsonl').\n            image_dir (str): Directory where the 'images/' folder is located.\n            clip_processor: The processor for CLIP images.\n            text_tokenizer: The tokenizer for MARBERT text.\n            max_len (int): Maximum sequence length for tokenization.\n        \"\"\"\n        self.data = load_dataset('json', data_files=jsonl_path)['train']\n        self.image_dir = image_dir\n        self.clip_processor = clip_processor\n        self.text_tokenizer = text_tokenizer\n        self.max_len = max_len\n        self.is_labels = is_labels\n\n        \n        # Map string labels to integers\n        # self.label_map = {\"not-hateful\": 0, \"hateful\": 1}\n        if self.is_labels and 'label' in self.data.column_names:\n            self.labels = self.data['label']\n        else:\n            self.labels = None\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # --- Load and Process Image ---\n        # --- Fix possible './' in img_path ---\n        img_path = item['img_path']\n        img_path = os.path.basename(img_path)\n    \n        image_path = os.path.join(self.image_dir, img_path)\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n            # Process image using CLIP's processor\n            image_processed = self.clip_processor(images=image, return_tensors=\"pt\")\n            pixel_values = image_processed['pixel_values'].squeeze(0) # Remove batch dim\n        except FileNotFoundError:\n            print(f\"Warning: Image not found at {image_path}. Using a dummy image.\")\n            # Provide a dummy tensor if an image is missing\n            pixel_values = torch.zeros((3, 224, 224))\n\n\n        # --- Load and Process Text ---\n        text = item['text']\n        # Tokenize text using MARBERT's tokenizer\n        text_tokenized = self.text_tokenizer(\n            text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        input_ids = text_tokenized['input_ids'].squeeze(0) # Remove batch dim\n        attention_mask = text_tokenized['attention_mask'].squeeze(0) # Remove batch dim\n        id_it = item['id']\n\n        #label = torch.tensor(item['label'], dtype=torch.long)\n\n        if not self.is_labels:\n            return {\n                'id': id_it,\n                'pixel_values': pixel_values,\n                'input_ids': input_ids,\n                'attention_mask': attention_mask\n            }\n        else:\n            label = torch.tensor(item['label'], dtype=torch.long)\n            return {\n                'pixel_values': pixel_values,\n                'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'label': label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_DATASET_DIR = \"./images\"\nTRAIN_FILE = \"arabic_hateful_meme_train.jsonl\"\nDEV_FILE = \"arabic_hateful_meme_dev.jsonl\"\nTEST_FILE = \"arabic_hateful_meme_test.jsonl\"\nTEST_NO_LABELS = \"task3_test_without_label.jsonl\"\n\nclip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nmarbert_tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n\ndef contrastive_loss(img_emb, txt_emb, logit_scale):\n    logits = logit_scale * img_emb @ txt_emb.t()   # (B, B)\n    labels = torch.arange(len(logits), device=logits.device)\n    loss_i2t = F.cross_entropy(logits, labels)\n    loss_t2i = F.cross_entropy(logits.t(), labels)\n    return (loss_i2t + loss_t2i) / 2\n\n\ntrain_dataset = Prop2HateMemeDataset(\n    jsonl_path=TRAIN_FILE,\n    image_dir=IMG_DATASET_DIR,\n    clip_processor=clip_processor,\n    text_tokenizer=marbert_tokenizer\n)\n\ndev_dataset = Prop2HateMemeDataset(\n    jsonl_path=DEV_FILE,\n    image_dir=IMG_DATASET_DIR,\n    clip_processor=clip_processor,\n    text_tokenizer=marbert_tokenizer\n)\n\ntest_dataset = Prop2HateMemeDataset(\n    jsonl_path=TEST_FILE,\n    image_dir=IMG_DATASET_DIR,\n    clip_processor=clip_processor,\n    text_tokenizer=marbert_tokenizer\n)\n\ntest_noLabels_dataset = Prop2HateMemeDataset(\n    jsonl_path=TEST_NO_LABELS,\n    image_dir=IMG_DATASET_DIR,\n    clip_processor=clip_processor,\n    text_tokenizer=marbert_tokenizer,\n)\n\ncombined_dataset = ConcatDataset([train_dataset, dev_dataset, test_dataset, test_noLabels_dataset])\nfull_data_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clip_vit = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nmarabert = AutoModel.from_pretrained(\"UBC-NLP/MARBERT\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CLIPArabic(clip_vit, marabert, proj_dim=512).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(\n    model: nn.Module,\n    data_loader: DataLoader,\n    epochs: int = 10,\n    lr: float = 5e-5,\n    weight_decay: float = 1e-2,\n    device: str = 'cuda'\n) -> nn.Module:\n    \n    # Setup device\n    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    model.train()\n\n    # Optimizer & Scheduler (only for trainable params)\n    optimizer = torch.optim.AdamW(\n        filter(lambda p: p.requires_grad, model.parameters()),\n        lr=lr,\n        weight_decay=weight_decay\n    )\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n\n    # Training loop\n    for epoch in range(1, epochs+1):\n        total_loss = 0.0\n        for batch in data_loader:\n            pixel_values = batch['pixel_values'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n\n            # Forward pass\n            img_emb, txt_emb, logit_scale = model(pixel_values, input_ids, attention_mask)\n            loss = contrastive_loss(img_emb, txt_emb, logit_scale)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        # Step scheduler\n        scheduler.step()\n\n        avg_loss = total_loss / len(data_loader)\n        print(f\"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f}\")\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = train(model, full_data_loader, epochs=80, lr=5e-5, weight_decay=1e-2, device='cuda')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/cust_clip.pt\")\nprint(f\"Saved final model state_dict to /kaggle/working/cust_clip.pt\")\n\nimport json\n\nmodel_clip = trained_model.to(device)\nmodel_clip.eval()\n\nembeddings_dict = {}\n\nfor batch in full_data_loader:\n    # Move data to device\n    pixel_values = batch[\"pixel_values\"].to(device)\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    ids = batch[\"id\"]  # List of IDs for this batch\n    \n    # Generate embeddings\n    with torch.no_grad():\n        img_embs, txt_embs, _ = model_clip(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n    \n    # Convert to lists and store\n    img_embs = img_embs.cpu().numpy().tolist()\n    txt_embs = txt_embs.cpu().numpy().tolist()\n    \n    for idx, id_val in enumerate(ids):\n        embeddings_dict[id_val] = {\n            \"image_embedding\": img_embs[idx],\n            \"text_embedding\": txt_embs[idx]\n        }\n\n# Save to JSON\nwith open(\"embeddings.json\", \"w\") as f:\n    json.dump(embeddings_dict, f, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}